# -*- coding: utf-8 -*-
"""House loan approval

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gNWh2dAJKJJUC6P45kItBxErHg-fYzGn
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from scipy import stats

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,precision_score, recall_score, roc_curve, auc, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import cross_val_score
from statistics import mean
import joblib

from warnings import filterwarnings
filterwarnings('ignore')
df = pd.read_csv("/content/drive/MyDrive/((GAssign) BankLoanApproval.csv")

X = df.drop('Default',axis=1)
#separte the predicting attribute into Y for model training
y = df['Default']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Further split training set into training and validation sets (60% train, 20% validation)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)


X_train.drop(['LoanID'],axis=1,inplace=True)
X_test.drop(['LoanID'],axis=1,inplace=True)
X_val.drop(['LoanID'],axis=1,inplace=True)

#Data Pre-processing
Education = {"High School": 0, "Bachelor's": 1, "Master's": 2, "PhD" : 3}
EmploymentType = {'Unemployed' : 0,'Self-employed' : 1, "Part-time": 2, "Full-time" : 3}
MaritalStatus = {'Single':0,'Married':1,'Divorced':2}
HasMortgage = {'No' : 0, 'Yes' : 1}
HasDependents = {'No' : 0, 'Yes' : 1}
LoanPurpose = {'Home' : 0, 'Education' : 1, "Auto": 2, "Business" : 3, "Other" : 4}
HasCoSigner = {'No' : 0, 'Yes' : 1}

X_train['Education'] = X_train['Education'].replace(Education)
X_train['EmploymentType'] = X_train['EmploymentType'].replace(EmploymentType)
X_train['MaritalStatus'] = X_train['MaritalStatus'].replace(MaritalStatus)
X_train['HasMortgage'] = X_train['HasMortgage'].replace(HasMortgage)
X_train['HasDependents'] = X_train['HasDependents'].replace(HasDependents)
X_train['LoanPurpose'] = X_train['LoanPurpose'].replace(LoanPurpose)
X_train['HasCoSigner'] = X_train['HasCoSigner'].replace(HasCoSigner)

X_test['Education'] = X_test['Education'].replace(Education)
X_test['EmploymentType'] = X_test['EmploymentType'].replace(EmploymentType)
X_test['MaritalStatus'] = X_test['MaritalStatus'].replace(MaritalStatus)
X_test['HasMortgage'] = X_test['HasMortgage'].replace(HasMortgage)
X_test['HasDependents'] = X_test['HasDependents'].replace(HasDependents)
X_test['LoanPurpose'] = X_test['LoanPurpose'].replace(LoanPurpose)
X_test['HasCoSigner'] = X_test['HasCoSigner'].replace(HasCoSigner)

X_val['Education'] = X_val['Education'].replace(Education)
X_val['EmploymentType'] = X_val['EmploymentType'].replace(EmploymentType)
X_val['MaritalStatus'] = X_val['MaritalStatus'].replace(MaritalStatus)
X_val['HasMortgage'] = X_val['HasMortgage'].replace(HasMortgage)
X_val['HasDependents'] = X_val['HasDependents'].replace(HasDependents)
X_val['LoanPurpose'] = X_val['LoanPurpose'].replace(LoanPurpose)
X_val['HasCoSigner'] = X_val['HasCoSigner'].replace(HasCoSigner)

# Feature scaling
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

#Naive Bayes
import joblib
from imblearn.over_sampling import SMOTE

param_grid_nb = {}
smote = SMOTE(random_state=42)
X_train, y_train= smote.fit_resample(X_train, y_train)
# Initialize the RandomizedSearchCV object for Gaussian Naive Bayes
random_search_nb = RandomizedSearchCV(GaussianNB(), param_distributions=param_grid_nb, n_iter=10, scoring='accuracy', cv=5, random_state=42)

# Fit the RandomizedSearchCV object for Gaussian Naive Bayes
random_search_nb.fit(X_train, y_train)

# Make predictions on the validation set using the best Gaussian Naive Bayes model
y_val_pred_nb = random_search_nb.predict(X_val)

# Calculate accuracy on the validation set for Gaussian Naive Bayes
accuracy_nb = accuracy_score(y_val, y_val_pred_nb)
print("Accuracy of Naive Bayes on validation set:", accuracy_nb)

# Generate classification report for Gaussian Naive Bayes on validation set
classification_report_nb = classification_report(y_val, y_val_pred_nb)
print("Classification Report of Naive Bayes on validation set:\n", classification_report_nb)

# Confusion Matrix for Gaussian Naive Bayes on validation set
cm_nb = confusion_matrix(y_val, y_val_pred_nb)
print("Confusion Matrix for Naive Bayes on validation set:\n", cm_nb)

#Decision Tree
param_grid_dt = {
    'criterion': ['gini', 'entropy'],         # Criterion for split
    'max_depth': [None, 5, 10, 20, 50],       # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],           # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]              # Minimum number of samples required to be at a leaf node
}
import joblib
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train, y_train= smote.fit_resample(X_train, y_train)

# Initialize the RandomizedSearchCV object for Decision Tree Classifier
random_search_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=param_grid_dt, n_iter=10, scoring='accuracy', cv=5, random_state=42)

# Fit the RandomizedSearchCV object for Decision Tree Classifier
random_search_dt.fit(X_train, y_train)

# Get the best Decision Tree model
best_dt = random_search_dt.best_estimator_

# Make predictions on the validation set using the best Decision Tree model
y_val_pred_dt = best_dt.predict(X_val)

# Calculate accuracy on the validation set for Decision Tree Classifier
accuracy_dt = accuracy_score(y_val, y_val_pred_dt)
print("Accuracy of Decision Tree on validation set:", accuracy_dt)

# Generate classification report for Decision Tree Classifier on validation set
classification_report_dt = classification_report(y_val, y_val_pred_dt)
print("Classification Report of Decision Tree on validation set:\n", classification_report_dt)

# Confusion Matrix for Decision Tree Classifier on validation set
cm_dt = confusion_matrix(y_val, y_val_pred_dt)
print("Confusion Matrix for Decision Tree on validation set:\n", cm_dt)



#Naive Bayes After selection
new_applicants_data = pd.read_csv('/content/drive/MyDrive/((GAssign) NewApplicants.csv')

new_applicants_data.drop(['LoanID'],axis=1,inplace=True)
new_applicants_data.drop(['Default'],axis=1,inplace=True)

#Data Pre-processing
Education = {"High School": 0, "Bachelor's": 1, "Master's": 2, "PhD" : 3}
EmploymentType = {'Unemployed' : 0,'Self-employed' : 1, "Part-time": 2, "Full-time" : 3}
MaritalStatus = {'Single':0,'Married':1,'Divorced':2}
HasMortgage = {'No' : 0, 'Yes' : 1}
HasDependents = {'No' : 0, 'Yes' : 1}
LoanPurpose = {'Home' : 0, 'Education' : 1, "Auto": 2, "Business" : 3, "Other" : 4}
HasCoSigner = {'No' : 0, 'Yes' : 1}

new_applicants_data['Education'] = new_applicants_data['Education'].replace(Education)
new_applicants_data['EmploymentType'] = new_applicants_data['EmploymentType'].replace(EmploymentType)
new_applicants_data['MaritalStatus'] = new_applicants_data['MaritalStatus'].replace(MaritalStatus)
new_applicants_data['HasMortgage'] = new_applicants_data['HasMortgage'].replace(HasMortgage)
new_applicants_data['HasDependents'] = new_applicants_data['HasDependents'].replace(HasDependents)
new_applicants_data['LoanPurpose'] = new_applicants_data['LoanPurpose'].replace(LoanPurpose)
new_applicants_data['HasCoSigner'] = new_applicants_data['HasCoSigner'].replace(HasCoSigner)


scaler = StandardScaler()

new_applicants_data = scaler.fit_transform(new_applicants_data)

#Store the best model
joblib.dump(random_search_nb, 'best_model.pkl')
#Load the best model
loaded_model = joblib.load('best_model.pkl')

# Use the loaded model to make predictions
predictions = loaded_model.predict(new_applicants_data)

print(predictions)

#Decision tree After selection
new_applicants_data = pd.read_csv('/content/drive/MyDrive/((GAssign) NewApplicants.csv')

new_applicants_data.drop(['LoanID'],axis=1,inplace=True)
new_applicants_data.drop(['Default'],axis=1,inplace=True)

#Data Pre-processing
Education = {"High School": 0, "Bachelor's": 1, "Master's": 2, "PhD" : 3}
EmploymentType = {'Unemployed' : 0,'Self-employed' : 1, "Part-time": 2, "Full-time" : 3}
MaritalStatus = {'Single':0,'Married':1,'Divorced':2}
HasMortgage = {'No' : 0, 'Yes' : 1}
HasDependents = {'No' : 0, 'Yes' : 1}
LoanPurpose = {'Home' : 0, 'Education' : 1, "Auto": 2, "Business" : 3, "Other" : 4}
HasCoSigner = {'No' : 0, 'Yes' : 1}

new_applicants_data['Education'] = new_applicants_data['Education'].replace(Education)
new_applicants_data['EmploymentType'] = new_applicants_data['EmploymentType'].replace(EmploymentType)
new_applicants_data['MaritalStatus'] = new_applicants_data['MaritalStatus'].replace(MaritalStatus)
new_applicants_data['HasMortgage'] = new_applicants_data['HasMortgage'].replace(HasMortgage)
new_applicants_data['HasDependents'] = new_applicants_data['HasDependents'].replace(HasDependents)
new_applicants_data['LoanPurpose'] = new_applicants_data['LoanPurpose'].replace(LoanPurpose)
new_applicants_data['HasCoSigner'] = new_applicants_data['HasCoSigner'].replace(HasCoSigner)


scaler = StandardScaler()

new_applicants_data = scaler.fit_transform(new_applicants_data)

#Store the best model
joblib.dump(random_search_dt, 'best_model.pkl')
#Load the best model
loaded_model = joblib.load('best_model.pkl')

# Use the loaded model to make predictions
predictions = loaded_model.predict(new_applicants_data)

print(predictions)



#tree decision
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from scipy import stats

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.metrics import classification_report, confusion_matrix, f1_score

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,precision_score, recall_score, roc_curve, auc, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import cross_val_score
from statistics import mean
import joblib
from imblearn.over_sampling import SMOTE

from warnings import filterwarnings
filterwarnings('ignore')
df = pd.read_csv("/content/drive/MyDrive/((GAssign) BankLoanApproval.csv")

X = df.drop('Default',axis=1)
#separte the predicting attribute into Y for model training
y = df['Default']

# Split dataset into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Further split training set into training and validation sets (60% train, 20% validation)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

#Data Pre-processing
Education = {"High School": 0, "Bachelor's": 1, "Master's": 2, "PhD" : 3}
EmploymentType = {'Unemployed' : 0,'Self-employed' : 1, "Part-time": 2, "Full-time" : 3}
MaritalStatus = {'Single':0,'Married':1,'Divorced':2}
HasMortgage = {'No' : 0, 'Yes' : 1}
HasDependents = {'No' : 0, 'Yes' : 1}
LoanPurpose = {'Home' : 0, 'Education' : 1, "Auto": 2, "Business" : 3, "Other" : 4}
HasCoSigner = {'No' : 0, 'Yes' : 1}

X_train['Education'] = X_train['Education'].replace(Education)
X_train['EmploymentType'] = X_train['EmploymentType'].replace(EmploymentType)
X_train['MaritalStatus'] = X_train['MaritalStatus'].replace(MaritalStatus)
X_train['HasMortgage'] = X_train['HasMortgage'].replace(HasMortgage)
X_train['HasDependents'] = X_train['HasDependents'].replace(HasDependents)
X_train['LoanPurpose'] = X_train['LoanPurpose'].replace(LoanPurpose)
X_train['HasCoSigner'] = X_train['HasCoSigner'].replace(HasCoSigner)

X_test['Education'] = X_test['Education'].replace(Education)
X_test['EmploymentType'] = X_test['EmploymentType'].replace(EmploymentType)
X_test['MaritalStatus'] = X_test['MaritalStatus'].replace(MaritalStatus)
X_test['HasMortgage'] = X_test['HasMortgage'].replace(HasMortgage)
X_test['HasDependents'] = X_test['HasDependents'].replace(HasDependents)
X_test['LoanPurpose'] = X_test['LoanPurpose'].replace(LoanPurpose)
X_test['HasCoSigner'] = X_test['HasCoSigner'].replace(HasCoSigner)

X_val['Education'] = X_val['Education'].replace(Education)
X_val['EmploymentType'] = X_val['EmploymentType'].replace(EmploymentType)
X_val['MaritalStatus'] = X_val['MaritalStatus'].replace(MaritalStatus)
X_val['HasMortgage'] = X_val['HasMortgage'].replace(HasMortgage)
X_val['HasDependents'] = X_val['HasDependents'].replace(HasDependents)
X_val['LoanPurpose'] = X_val['LoanPurpose'].replace(LoanPurpose)
X_val['HasCoSigner'] = X_val['HasCoSigner'].replace(HasCoSigner)

X_train.drop(['LoanID'],axis=1,inplace=True)
X_test.drop(['LoanID'],axis=1,inplace=True)
X_val.drop(['LoanID'],axis=1,inplace=True)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)
#Tree decision
param_dist = {
    'criterion': ['gini', 'entropy'],         # Criterion for split
    'max_depth': [None, 5, 10, 20, 50],       # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],           # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]              # Minimum number of samples required to be at a leaf node
}


smote = SMOTE(random_state=42)
X_train, y_train= smote.fit_resample(X_train, y_train)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# Lists to store evaluation results
accuracy_scores = []
f1_scores = []

# Initialize the RandomizedSearchCV object
random_search = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=param_dist,
                                   n_iter=10, scoring='accuracy', cv=5, random_state=42)
# Perform cross-validation
for train_index, val_index in skf.split(X_train, y_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

    # Fit the RandomizedSearchCV object within each fold
    random_search.fit(X_train_fold, y_train_fold)

    # Get the best Logistic Regression model for this fold
    best_td_model_fold = random_search.best_estimator_

    # Make predictions on the validation set using the best model for this fold
    y_val_pred_fold = best_td_model_fold.predict(X_val_fold)

    # Calculate evaluation metrics for this fold
    accuracy_fold = accuracy_score(y_val_fold, y_val_pred_fold)
    f1_fold = f1_score(y_val_fold, y_val_pred_fold)

    # Append evaluation metrics to lists
    accuracy_scores.append(accuracy_fold)
    f1_scores.append(f1_fold)

# Calculate mean accuracy and F1 score across folds
mean_accuracy = np.mean(accuracy_scores)
mean_f1_score = np.mean(f1_scores)

# Print mean accuracy and F1 score across folds
print("Mean Accuracy across Folds:", mean_accuracy)
print("Mean F1 Score across Folds:", mean_f1_score)


# Make predictions on the validation set using the best model
y_val_pred = best_td_model_fold.predict(X_val)

# Calculate accuracy on validation set
accuracy_val = accuracy_score(y_val_fold, y_val_pred_fold)
print("Accuracy on Validation Set:", accuracy_val)

# Print classification report on validation set
print("\nClassification Report on Validation Set:\n")
print(classification_report(y_val_fold, y_val_pred_fold))

# Print confusion matrix on validation set
print("\nConfusion Matrix on Validation Set:\n")
cm_val = confusion_matrix(y_val_fold, y_val_pred_fold)
print(cm_val)